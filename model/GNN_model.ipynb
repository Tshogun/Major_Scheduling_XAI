{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d861b283",
   "metadata": {},
   "source": [
    "# GNN Timetabling — Full Notebook\n",
    "\n",
    "This notebook trains a **simple Graph Neural Network (GNN)** in PyTorch to predict timetables from JSON instances stored in `../database/instance_*.json` and writes predictions back in the same JSON-like timetable format.\n",
    "\n",
    "**High-level flow**:\n",
    "1. Load JSON instances\n",
    "2. Convert each instance into per-(class,day,slot) examples and build a global vocabulary of feasible `(subject,teacher)` options\n",
    "3. Train a supervised model to classify each slot into one of the vocab options\n",
    "4. Decode predictions back into a timetable JSON and save to `/mnt/data/predicted`\n",
    "\n",
    "> The model is a practical starter template. Replace or extend the GNN and decoding logic for better performance and constraint handling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4199646c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python and PyTorch ready. torch: 2.7.1+cu118\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Basic imports and environment checks\n",
    "import os, glob, json, math, random, copy\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "from collections import defaultdict, Counter\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pprint\n",
    "print('Python and PyTorch ready. torch:', torch.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d3e97d",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "Set paths, device, and hyperparameters here. Adjust `DATABASE_DIR` to point at your JSON files (relative to notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c700737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "DATABASE_DIR = '../dataset'  # Where instance_001.json ... live\n",
    "PREDICT_DIR = '/mnt/data/predicted'\n",
    "os.makedirs(PREDICT_DIR, exist_ok=True)\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 12\n",
    "EMBED_DIM = 128\n",
    "LR = 1e-3\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "print('Device:', DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3c4c70",
   "metadata": {},
   "source": [
    "## 1) Load instances\n",
    "Loads JSON instances from DATABASE_DIR and shows an example. The notebook expects files named like `instance_001.json`. If none are found, add example JSONs there and re-run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "997f02d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 200 instances in ../dataset\n",
      "\n",
      "Example instance keys: ['input', 'output']\n",
      "\n",
      "Input summary:\n",
      "{'num_classes': 4,\n",
      " 'num_days': 6,\n",
      " 'num_subjects': 10,\n",
      " 'num_teachers': 7,\n",
      " 'slots_per_day': 7}\n"
     ]
    }
   ],
   "source": [
    "def load_instances(db_dir):\n",
    "    paths = sorted(glob.glob(os.path.join(db_dir, 'instance_*.json')))\n",
    "    instances = []\n",
    "    for p in paths:\n",
    "        with open(p, 'r') as f:\n",
    "            try:\n",
    "                instances.append(json.load(f))\n",
    "            except Exception as e:\n",
    "                print('Failed to load', p, '-', e)\n",
    "    return instances\n",
    "\n",
    "instances = load_instances(DATABASE_DIR)\n",
    "print(f'Found {len(instances)} instances in', DATABASE_DIR)\n",
    "if instances:\n",
    "    print('\\nExample instance keys:', list(instances[0].keys()))\n",
    "    pprint = __import__('pprint').pprint\n",
    "    print('\\nInput summary:')\n",
    "    pprint({k:v for k,v in instances[0]['input'].items() if k!='class_subjects' and k!='teacher_subjects' and k!='subject_sessions_per_week'})\n",
    "else:\n",
    "    print('No instances found. Place JSON files in', DATABASE_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0defbce",
   "metadata": {},
   "source": [
    "## 2) Convert instances → per-slot examples\n",
    "We transform each instance into examples for every (class, day, slot). Each example contains the feasible `(subject,teacher)` options and the ground-truth label if present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb4b612e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example count for instance 0: 168\n",
      "First example: {'class': 0, 'day': 0, 'slot': 0, 'options': [(3, 1), (3, 2), (3, 3), (3, 6), (8, 0), (8, 1), (8, 2), (8, 3), (1, 0), (1, 1), (1, 2), (1, 3), (7, 1), (7, 2), (7, 3), (6, 1), (6, 2), (6, 5), (6, 6), (0, 0), (0, 1), (0, 2), (0, 4), (2, 1), (2, 2)], 'label': (6, 5)}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def parse_sspw(sspw_raw):\n",
    "    # Converts keys like '(0, 6)' or '0,6' to tuple keys\n",
    "    out = {}\n",
    "    for k,v in sspw_raw.items():\n",
    "        ks = k\n",
    "        if isinstance(k, str):\n",
    "            ks = k.strip()\n",
    "            if ks.startswith('(') and ks.endswith(')'):\n",
    "                ks = ks[1:-1]\n",
    "            parts = [p.strip() for p in ks.split(',')]\n",
    "            if len(parts) == 2:\n",
    "                a,b = int(parts[0]), int(parts[1])\n",
    "                out[(a,b)] = int(v)\n",
    "            else:\n",
    "                # fallback, ignore\n",
    "                pass\n",
    "        else:\n",
    "            # unexpected format\n",
    "            pass\n",
    "    return out\n",
    "\n",
    "def instance_to_examples(instance):\n",
    "    inp = instance['input']\n",
    "    out = instance.get('output', {})\n",
    "    num_classes = int(inp['num_classes'])\n",
    "    num_days = int(inp['num_days'])\n",
    "    slots_per_day = int(inp['slots_per_day'])\n",
    "    # ensure keys are ints for class_subjects and teacher_subjects\n",
    "    class_subjects = {int(k): v for k,v in inp['class_subjects'].items()}\n",
    "    teacher_subjects = {int(k): v for k,v in inp['teacher_subjects'].items()}\n",
    "    sspw = parse_sspw(inp.get('subject_sessions_per_week', {}))\n",
    "    examples = []\n",
    "    for c in range(num_classes):\n",
    "        class_grid = out.get(str(c), None)\n",
    "        for d in range(num_days):\n",
    "            for s in range(slots_per_day):\n",
    "                label = None\n",
    "                if class_grid is not None:\n",
    "                    try:\n",
    "                        pair = class_grid[d][s]\n",
    "                        label = (int(pair[0]), int(pair[1]))\n",
    "                    except Exception:\n",
    "                        label = (-1,-1)\n",
    "                # feasible options: subject in class_subjects[c] and teacher qualified\n",
    "                options = []\n",
    "                for subj in class_subjects.get(c, []):\n",
    "                    for t, quals in teacher_subjects.items():\n",
    "                        if subj in quals:\n",
    "                            options.append((int(subj), int(t)))\n",
    "                if not options:\n",
    "                    options = [(-1,-1)]\n",
    "                examples.append({\n",
    "                    'class': c,\n",
    "                    'day': d,\n",
    "                    'slot': s,\n",
    "                    'options': options,\n",
    "                    'label': label\n",
    "                })\n",
    "    return examples\n",
    "\n",
    "# quick sanity check (if instances available)\n",
    "if instances:\n",
    "    exs = instance_to_examples(instances[0])\n",
    "    print('Example count for instance 0:', len(exs))\n",
    "    print('First example:', exs[0])\n",
    "else:\n",
    "    print('No instances to convert yet.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962947fe",
   "metadata": {},
   "source": [
    "## 3) Build global vocabulary of (subject,teacher) options\n",
    "We'll create a mapping from observed `(subject,teacher)` pairs to integer class labels the model will predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d39606f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 120\n",
      "Some vocab items: [((0, 0), 0), ((0, 1), 1), ((0, 2), 2), ((0, 3), 3), ((0, 4), 4), ((0, 5), 5), ((0, 6), 6), ((0, 7), 7), ((0, 8), 8), ((0, 9), 9)]\n"
     ]
    }
   ],
   "source": [
    "def build_option_vocab(all_instances):\n",
    "    counter = Counter()\n",
    "    for inst in all_instances:\n",
    "        for ex in instance_to_examples(inst):\n",
    "            for opt in ex['options']:\n",
    "                counter[tuple(opt)] += 1\n",
    "            if ex['label'] is not None and ex['label'] != (-1,-1):\n",
    "                counter[tuple(ex['label'])] += 1\n",
    "    # sort to have deterministic ordering\n",
    "    opts = sorted(counter.keys(), key=lambda x:(x[0], x[1]))\n",
    "    vocab = {opt:i for i,opt in enumerate(opts)}\n",
    "    vocab_inv = {i:opt for opt,i in vocab.items()}\n",
    "    return vocab, vocab_inv\n",
    "\n",
    "if instances:\n",
    "    vocab, vocab_inv = build_option_vocab(instances)\n",
    "    print('Vocab size:', len(vocab))\n",
    "    print('Some vocab items:', list(vocab.items())[:10])\n",
    "else:\n",
    "    vocab, vocab_inv = {}, {}\n",
    "    print('Empty vocab (no instances)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca87e125",
   "metadata": {},
   "source": [
    "## 4) Dataset and DataLoader\n",
    "The Dataset yields per-slot samples. We include metadata in each sample so the model can access instance-level sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63c92440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch keys: ['class_ids', 'day_ids', 'slot_ids', 'label_idxs', 'options', 'meta', 'raw_batch']\n",
      "class_ids shape: torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "class TimetablingDataset(Dataset):\n",
    "    def __init__(self, instances, vocab):\n",
    "        self.rows = []\n",
    "        self.vocab = vocab\n",
    "        for inst in instances:\n",
    "            exs = instance_to_examples(inst)\n",
    "            for ex in exs:\n",
    "                self.rows.append((inst['input'], ex))\n",
    "    def __len__(self):\n",
    "        return len(self.rows)\n",
    "    def __getitem__(self, idx):\n",
    "        inp, ex = self.rows[idx]\n",
    "        sample = {\n",
    "            'meta': inp,\n",
    "            'class': ex['class'],\n",
    "            'day': ex['day'],\n",
    "            'slot': ex['slot'],\n",
    "            'options': ex['options'],\n",
    "            'label': ex['label']\n",
    "        }\n",
    "        # map label to vocab idx\n",
    "        if sample['label'] is None:\n",
    "            sample['label_idx'] = -1\n",
    "        else:\n",
    "            sample['label_idx'] = self.vocab.get(tuple(sample['label']), -1)\n",
    "        return sample\n",
    "\n",
    "def collate_fn(batch):\n",
    "    meta = batch[0]['meta']\n",
    "    num_teachers = int(meta['num_teachers'])\n",
    "    class_ids = torch.tensor([b['class'] for b in batch], dtype=torch.long)\n",
    "    day_ids = torch.tensor([b['day'] for b in batch], dtype=torch.long)\n",
    "    slot_ids = torch.tensor([b['slot'] for b in batch], dtype=torch.long)\n",
    "    label_idxs = torch.tensor([b['label_idx'] for b in batch], dtype=torch.long)\n",
    "    options = [b['options'] for b in batch]\n",
    "    meta_out = {'num_teachers': num_teachers}\n",
    "    return {\n",
    "        'class_ids': class_ids,\n",
    "        'day_ids': day_ids,\n",
    "        'slot_ids': slot_ids,\n",
    "        'label_idxs': label_idxs,\n",
    "        'options': options,\n",
    "        'meta': meta_out,\n",
    "        'raw_batch': batch\n",
    "    }\n",
    "\n",
    "# quick dataset test\n",
    "if instances:\n",
    "    dataset = TimetablingDataset(instances, vocab)\n",
    "    loader = DataLoader(dataset, batch_size=8, collate_fn=collate_fn)\n",
    "    batch = next(iter(loader))\n",
    "    print('Batch keys:', list(batch.keys()))\n",
    "    print('class_ids shape:', batch['class_ids'].shape)\n",
    "else:\n",
    "    print('No dataset to create.')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff2d01b",
   "metadata": {},
   "source": [
    "## 5) Simple GNN model\n",
    "A straightforward embedding-based model that concatenates class/day/slot embeddings and a simple aggregated teacher embedding. The model outputs logits over the global option vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0072d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SimpleGNN(nn.Module):\n",
    "    def __init__(self, num_classes, num_teachers, num_days, slots_per_day, embed_dim, vocab_size):\n",
    "        super().__init__()\n",
    "        self.class_emb = nn.Embedding(max(1,num_classes), embed_dim)\n",
    "        self.day_emb = nn.Embedding(max(1,num_days), embed_dim)\n",
    "        self.slot_emb = nn.Embedding(max(1,slots_per_day), embed_dim)\n",
    "        self.teacher_emb = nn.Embedding(max(1, num_teachers), embed_dim)\n",
    "        self.msg_fc = nn.Sequential(\n",
    "            nn.Linear(embed_dim*4, embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_dim//2, max(1, vocab_size))\n",
    "        )\n",
    "\n",
    "    def forward(self, class_ids, day_ids, slot_ids, meta):\n",
    "        c_emb = self.class_emb(class_ids)\n",
    "        d_emb = self.day_emb(day_ids)\n",
    "        s_emb = self.slot_emb(slot_ids)\n",
    "        # teacher aggregate (mean of teacher embeddings)\n",
    "        t_count = meta['num_teachers']\n",
    "        t_idx = torch.arange(t_count, device=class_ids.device)\n",
    "        t_emb = self.teacher_emb(t_idx)  # [T, D]\n",
    "        t_mean = t_emb.mean(dim=0, keepdim=True)  # [1, D]\n",
    "        t_mean_expand = t_mean.expand(c_emb.size(0), -1)\n",
    "        x = torch.cat([c_emb, d_emb, s_emb, t_mean_expand], dim=1)\n",
    "        x = self.msg_fc(x)\n",
    "        logits = self.classifier(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15d8006c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 'negative label_idx -1'), (5, 'negative label_idx -1'), (7, 'negative label_idx -1'), (10, 'negative label_idx -1'), (17, 'negative label_idx -1'), (22, 'negative label_idx -1'), (27, 'negative label_idx -1'), (43, 'negative label_idx -1'), (44, 'negative label_idx -1'), (46, 'negative label_idx -1'), (47, 'negative label_idx -1'), (48, 'negative label_idx -1'), (49, 'negative label_idx -1'), (50, 'negative label_idx -1'), (51, 'negative label_idx -1'), (52, 'negative label_idx -1'), (53, 'negative label_idx -1'), (56, 'negative label_idx -1'), (59, 'negative label_idx -1'), (60, 'negative label_idx -1'), (61, 'negative label_idx -1'), (64, 'negative label_idx -1'), (65, 'negative label_idx -1'), (67, 'negative label_idx -1'), (68, 'negative label_idx -1'), (69, 'negative label_idx -1'), (72, 'negative label_idx -1'), (73, 'negative label_idx -1'), (79, 'negative label_idx -1'), (82, 'negative label_idx -1'), (83, 'negative label_idx -1'), (84, 'negative label_idx -1'), (85, 'negative label_idx -1'), (86, 'negative label_idx -1'), (87, 'negative label_idx -1'), (88, 'negative label_idx -1'), (89, 'negative label_idx -1'), (90, 'negative label_idx -1'), (91, 'negative label_idx -1'), (92, 'negative label_idx -1'), (93, 'negative label_idx -1'), (94, 'negative label_idx -1'), (95, 'negative label_idx -1'), (96, 'negative label_idx -1'), (97, 'negative label_idx -1'), (98, 'negative label_idx -1'), (99, 'negative label_idx -1'), (101, 'negative label_idx -1'), (102, 'negative label_idx -1'), (103, 'negative label_idx -1')]\n"
     ]
    }
   ],
   "source": [
    "def dataset_index_checks(dataset, max_class_id, max_day_id, max_slot_id):\n",
    "    problems = []\n",
    "    for i, x in enumerate(dataset):\n",
    "        for name, allowed_max in [('class', max_class_id),\n",
    "                                  ('day', max_day_id),\n",
    "                                  ('slot', max_slot_id)]:\n",
    "            if name not in x:\n",
    "                problems.append((i, f\"missing key {name}\"))\n",
    "                continue\n",
    "            v = x[name]\n",
    "            if isinstance(v, int):\n",
    "                minv = maxv = v\n",
    "            else:\n",
    "                v_t = torch.as_tensor(v)\n",
    "                if v_t.numel() == 0:\n",
    "                    problems.append((i, f\"{name} empty\"))\n",
    "                    continue\n",
    "                minv, maxv = int(v_t.min().item()), int(v_t.max().item())\n",
    "            if minv < 0 or maxv > allowed_max:\n",
    "                problems.append((i, f\"{name} indices {minv}-{maxv} out of allowed 0-{allowed_max}\"))\n",
    "\n",
    "        if 'label_idx' in x:\n",
    "            lblv = int(x['label_idx']) if not isinstance(x['label_idx'], (list, tuple)) else int(x['label_idx'][0])\n",
    "            if lblv < 0:\n",
    "                problems.append((i, f\"negative label_idx {lblv}\"))\n",
    "    return problems\n",
    "\n",
    "# Use your earlier printed maxima\n",
    "probs = dataset_index_checks(dataset, max_class_id=7, max_day_id=5, max_slot_id=7)\n",
    "print(probs[:50] if probs else \"No problems found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afa7858",
   "metadata": {},
   "source": [
    "## 6) Training loop\n",
    "Train the model with CrossEntropy over the global vocab. We ignore unknown labels (`-1`) during loss computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ca41bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_class_id: 7\n",
      "max_day_id: 5\n",
      "max_slot_id: 7\n",
      "num_labels (output classes): 120\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "SimpleGNN.__init__() got an unexpected keyword argument 'num_outputs'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 125\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# Run training if instances exist\u001b[39;00m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m instances:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     model, dataset, vocab = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstances\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    127\u001b[39m     model, dataset = \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 76\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(instances, vocab, num_epochs, batch_size)\u001b[39m\n\u001b[32m     73\u001b[39m num_teachers = instances[\u001b[32m0\u001b[39m].get(\u001b[33m\"\u001b[39m\u001b[33mmeta\u001b[39m\u001b[33m\"\u001b[39m, {}).get(\u001b[33m\"\u001b[39m\u001b[33mnum_teachers\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m     75\u001b[39m \u001b[38;5;66;03m# ✅ Removed num_slots (not in SimpleGNN)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m model = \u001b[43mSimpleGNN\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_class_id\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_days\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_day_id\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_teachers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_teachers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_labels\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m.to(DEVICE)\n\u001b[32m     84\u001b[39m optimizer = torch.optim.Adam(model.parameters(), lr=\u001b[32m1e-3\u001b[39m)\n\u001b[32m     85\u001b[39m criterion = nn.CrossEntropyLoss()\n",
      "\u001b[31mTypeError\u001b[39m: SimpleGNN.__init__() got an unexpected keyword argument 'num_outputs'"
     ]
    }
   ],
   "source": [
    "def train_model(instances, vocab):\n",
    "    # Compute dataset-level stats\n",
    "    max_class_id = max(slot[\"class\"] for inst in instances for slot in inst[\"slots\"])\n",
    "    max_day_id   = max(slot[\"day\"] for inst in instances for slot in inst[\"slots\"])\n",
    "    max_slot_id  = max(slot[\"slot\"] for inst in instances for slot in inst[\"slots\"])\n",
    "    num_labels   = len(vocab)\n",
    "\n",
    "    print(f\"max_class_id: {max_class_id}\")\n",
    "    print(f\"max_day_id: {max_day_id}\")\n",
    "    print(f\"max_slot_id: {max_slot_id}\")\n",
    "    print(f\"num_labels (output classes): {num_labels}\")\n",
    "\n",
    "    # Extract num_teachers safely\n",
    "    num_teachers = instances[0].get(\"meta\", {}).get(\"num_teachers\", 1)\n",
    "\n",
    "    # ✅ Fixed: removed num_outputs (not expected by SimpleGNN)\n",
    "    model = SimpleGNN(\n",
    "        num_classes=max_class_id + 1,\n",
    "        num_days=max_day_id + 1,\n",
    "        num_teachers=num_teachers,\n",
    "        vocab_size=len(vocab)\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    dataset = TimetablingDataset(instances, vocab)\n",
    "    dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "    for epoch in range(5):  # you can increase this later\n",
    "        total_loss = 0\n",
    "        for class_ids, day_ids, slot_ids, teacher_ids, labels, meta_batch in dataloader:\n",
    "            class_ids, day_ids, slot_ids, teacher_ids, labels = (\n",
    "                class_ids.to(DEVICE),\n",
    "                day_ids.to(DEVICE),\n",
    "                slot_ids.to(DEVICE),\n",
    "                teacher_ids.to(DEVICE),\n",
    "                labels.to(DEVICE)\n",
    "            )\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(class_ids, day_ids, slot_ids, meta_batch)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Loss = {total_loss/len(dataloader):.4f}\")\n",
    "\n",
    "    return model, dataset, vocab\n",
    "def train_model(instances, vocab, num_epochs=5, batch_size=32):\n",
    "    dataset = TimetablingDataset(instances, vocab)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Compute max ids from dataset tensors (using correct keys)\n",
    "    all_class_ids = torch.tensor([item[\"class\"] for item in dataset])\n",
    "    all_day_ids   = torch.tensor([item[\"day\"] for item in dataset])\n",
    "    all_slot_ids  = torch.tensor([item[\"slot\"] for item in dataset])\n",
    "    all_labels    = torch.tensor([item[\"label_idx\"] for item in dataset])\n",
    "\n",
    "    max_class_id = all_class_ids.max().item()\n",
    "    max_day_id   = all_day_ids.max().item()\n",
    "    max_slot_id  = all_slot_ids.max().item()\n",
    "    num_labels   = all_labels.max().item() + 1\n",
    "\n",
    "    print(f\"max_class_id: {max_class_id}\")\n",
    "    print(f\"max_day_id: {max_day_id}\")\n",
    "    print(f\"max_slot_id: {max_slot_id}\")\n",
    "    print(f\"num_labels (output classes): {num_labels}\")\n",
    "\n",
    "    # Extract num_teachers safely from instances\n",
    "    num_teachers = instances[0].get(\"meta\", {}).get(\"num_teachers\", 1)\n",
    "\n",
    "    # ✅ Removed num_slots (not in SimpleGNN)\n",
    "    model = SimpleGNN(\n",
    "        num_classes=max_class_id + 1,\n",
    "        num_days=max_day_id + 1,\n",
    "        num_teachers=num_teachers,\n",
    "        vocab_size=len(vocab),\n",
    "        num_outputs=num_labels\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            class_ids = batch['class'].to(DEVICE)\n",
    "            day_ids   = batch['day'].to(DEVICE)\n",
    "            slot_ids  = batch['slot'].to(DEVICE)\n",
    "            labels    = batch['label_idx'].to(DEVICE)\n",
    "            meta_batch = batch['meta']\n",
    "\n",
    "            # Debug info (first batch only)\n",
    "            if batch_idx == 0 and epoch == 0:\n",
    "                print(\"\\nSanity check on first batch:\")\n",
    "                print(\"class_ids range:\", class_ids.min().item(), \"-\", class_ids.max().item())\n",
    "                print(\"day_ids range:\", day_ids.min().item(), \"-\", day_ids.max().item())\n",
    "                print(\"slot_ids range:\", slot_ids.min().item(), \"-\", slot_ids.max().item())\n",
    "                print(\"labels range:\", labels.min().item(), \"-\", labels.max().item())\n",
    "\n",
    "            # Safety clamp for labels\n",
    "            if labels.min() < 0 or labels.max() >= num_labels:\n",
    "                print(f\"⚠️ Invalid labels detected in batch {batch_idx}, clamping.\")\n",
    "                labels = labels.clamp(0, num_labels - 1)\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model(class_ids, day_ids, slot_ids, meta_batch)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            # Backprop\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "    return model, dataset, vocab\n",
    "\n",
    "\n",
    "# Run training if instances exist\n",
    "if instances:\n",
    "    model, dataset, vocab = train_model(instances, vocab)\n",
    "else:\n",
    "    model, dataset = None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc7d091",
   "metadata": {},
   "source": [
    "## 7) Inference and export\n",
    "Predict for each slot and write predictions to JSON files under `/mnt/data/predicted/` with names `predicted_instance_000.json`, etc.\n",
    "\n",
    "**Note**: this decoding simply picks the most probable `(subject,teacher)` pair from the global vocab for each slot. For stricter feasibility, add masking and a repair step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f52b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_class_id: 7\n",
      "max_day_id: 5\n",
      "max_slot_id: 7\n",
      "num_labels (output classes): 120\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'TimetablingDataset' object has no attribute 'meta'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 71\u001b[39m\n",
      "\u001b[32m     69\u001b[39m \u001b[38;5;66;03m# Run training if instances exist\u001b[39;00m\n",
      "\u001b[32m     70\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m instances:\n",
      "\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m     model, dataset, vocab = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstances\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m     72\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m     73\u001b[39m     model, dataset = \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(instances, vocab, num_epochs, batch_size)\u001b[39m\n",
      "\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmax_slot_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_slot_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mnum_labels (output classes): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_labels\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[32m     21\u001b[39m model = SimpleGNN(\n",
      "\u001b[32m     22\u001b[39m     num_classes=max_class_id + \u001b[32m1\u001b[39m,\n",
      "\u001b[32m     23\u001b[39m     num_days=max_day_id + \u001b[32m1\u001b[39m,\n",
      "\u001b[32m     24\u001b[39m     num_slots=max_slot_id + \u001b[32m1\u001b[39m,\n",
      "\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     num_teachers=\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmeta\u001b[49m.get(\u001b[33m'\u001b[39m\u001b[33mnum_teachers\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m1\u001b[39m),  \u001b[38;5;66;03m# safer access\u001b[39;00m\n",
      "\u001b[32m     26\u001b[39m     vocab_size=\u001b[38;5;28mlen\u001b[39m(vocab),\n",
      "\u001b[32m     27\u001b[39m     num_outputs=num_labels\n",
      "\u001b[32m     28\u001b[39m ).to(DEVICE)\n",
      "\u001b[32m     30\u001b[39m optimizer = torch.optim.Adam(model.parameters(), lr=\u001b[32m1e-3\u001b[39m)\n",
      "\u001b[32m     31\u001b[39m criterion = nn.CrossEntropyLoss()\n",
      "\n",
      "\u001b[31mAttributeError\u001b[39m: 'TimetablingDataset' object has no attribute 'meta'"
     ]
    }
   ],
   "source": [
    "def train_model(instances, vocab, num_epochs=5, batch_size=32):\n",
    "    dataset = TimetablingDataset(instances, vocab)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Compute max ids from dataset tensors (using correct keys)\n",
    "    all_class_ids = torch.tensor([item[\"class\"] for item in dataset])\n",
    "    all_day_ids   = torch.tensor([item[\"day\"] for item in dataset])\n",
    "    all_slot_ids  = torch.tensor([item[\"slot\"] for item in dataset])\n",
    "    all_labels    = torch.tensor([item[\"label_idx\"] for item in dataset])\n",
    "\n",
    "    max_class_id = all_class_ids.max().item()\n",
    "    max_day_id   = all_day_ids.max().item()\n",
    "    max_slot_id  = all_slot_ids.max().item()\n",
    "    num_labels   = all_labels.max().item() + 1\n",
    "\n",
    "    print(f\"max_class_id: {max_class_id}\")\n",
    "    print(f\"max_day_id: {max_day_id}\")\n",
    "    print(f\"max_slot_id: {max_slot_id}\")\n",
    "    print(f\"num_labels (output classes): {num_labels}\")\n",
    "\n",
    "    model = SimpleGNN(\n",
    "        num_classes=max_class_id + 1,\n",
    "        num_days=max_day_id + 1,\n",
    "        num_slots=max_slot_id + 1,\n",
    "        num_teachers=dataset.meta.get('num_teachers', 1),  # safer access\n",
    "        vocab_size=len(vocab),\n",
    "        num_outputs=num_labels\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            class_ids = batch['class'].to(DEVICE)\n",
    "            day_ids   = batch['day'].to(DEVICE)\n",
    "            slot_ids  = batch['slot'].to(DEVICE)\n",
    "            labels    = batch['label_idx'].to(DEVICE)\n",
    "            meta_batch = batch['meta']\n",
    "\n",
    "            # Debug info (first batch only)\n",
    "            if batch_idx == 0 and epoch == 0:\n",
    "                print(\"\\nSanity check on first batch:\")\n",
    "                print(\"class_ids range:\", class_ids.min().item(), \"-\", class_ids.max().item())\n",
    "                print(\"day_ids range:\", day_ids.min().item(), \"-\", day_ids.max().item())\n",
    "                print(\"slot_ids range:\", slot_ids.min().item(), \"-\", slot_ids.max().item())\n",
    "                print(\"labels range:\", labels.min().item(), \"-\", labels.max().item())\n",
    "\n",
    "            # Safety clamp for labels\n",
    "            if labels.min() < 0 or labels.max() >= num_labels:\n",
    "                print(f\"⚠️ Invalid labels detected in batch {batch_idx}, clamping.\")\n",
    "                labels = labels.clamp(0, num_labels - 1)\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model(class_ids, day_ids, slot_ids, meta_batch)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            # Backprop\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "    return model, dataset, vocab\n",
    "\n",
    "\n",
    "# Run training if instances exist\n",
    "if instances:\n",
    "    model, dataset, vocab = train_model(instances, vocab)\n",
    "else:\n",
    "    model, dataset = None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d013f957",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     23\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m'\u001b[39m\u001b[33minput\u001b[39m\u001b[33m'\u001b[39m: meta, \u001b[33m'\u001b[39m\u001b[33moutput\u001b[39m\u001b[33m'\u001b[39m: output}\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Run prediction for all instances and save\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m instances \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mmodel\u001b[49m:\n\u001b[32m     27\u001b[39m     os.makedirs(PREDICT_DIR, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     28\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, inst \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(instances):\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "def predict_instance(model, instance, vocab_inv):\n",
    "    meta = instance['input']\n",
    "    examples = instance_to_examples(instance)\n",
    "    device = DEVICE\n",
    "    model.eval()\n",
    "    num_classes = int(meta['num_classes'])\n",
    "    num_days = int(meta['num_days'])\n",
    "    slots_per_day = int(meta['slots_per_day'])\n",
    "    output = {str(c): [[[-1,-1] for _ in range(slots_per_day)] for _ in range(num_days)] for c in range(num_classes)}\n",
    "    with torch.no_grad():\n",
    "        for ex in examples:\n",
    "            class_id = torch.tensor([ex['class']], dtype=torch.long, device=device)\n",
    "            day_id = torch.tensor([ex['day']], dtype=torch.long, device=device)\n",
    "            slot_id = torch.tensor([ex['slot']], dtype=torch.long, device=device)\n",
    "            logits = model(class_id, day_id, slot_id, {'num_teachers': meta['num_teachers']})\n",
    "            if logits.size(1) == 1:\n",
    "                choice_idx = 0\n",
    "            else:\n",
    "                probs = torch.softmax(logits, dim=1).cpu().numpy()[0]\n",
    "                choice_idx = int(probs.argmax())\n",
    "            pair = vocab_inv.get(choice_idx, (-1,-1))\n",
    "            output[str(ex['class'])][ex['day']][ex['slot']] = [int(pair[0]), int(pair[1])]\n",
    "    return {'input': meta, 'output': output}\n",
    "\n",
    "# Run prediction for all instances and save\n",
    "if instances and model:\n",
    "    os.makedirs(PREDICT_DIR, exist_ok=True)\n",
    "    for i, inst in enumerate(instances):\n",
    "        pred = predict_instance(model, inst, vocab_inv)\n",
    "        out_path = os.path.join(PREDICT_DIR, f'predicted_instance_{i:03d}.json')\n",
    "        with open(out_path, 'w') as f:\n",
    "            json.dump(pred, f, indent=2)\n",
    "    print('Predictions saved to', PREDICT_DIR)\n",
    "else:\n",
    "    print('No model or instances available for prediction.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea9dc3a",
   "metadata": {},
   "source": [
    "## 8) Basic evaluation metrics\n",
    "We compute per-slot accuracy (fraction of slots where the predicted (subject,teacher) equals the ground truth) and average per-instance Hamming similarity (normalized)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5267b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_predictions(instances, predict_dir):\n",
    "    pred_paths = sorted(glob.glob(os.path.join(predict_dir, 'predicted_instance_*.json')))\n",
    "    if not pred_paths:\n",
    "        print('No predicted files found in', predict_dir); return\n",
    "    accuracies = []\n",
    "    hamming_scores = []\n",
    "    for i, p in enumerate(pred_paths):\n",
    "        with open(p,'r') as f:\n",
    "            pred = json.load(f)\n",
    "        gold = instances[i].get('output', {})\n",
    "        # compare per-slot exact matches (excluding missing gold)\n",
    "        total = 0; correct = 0\n",
    "        total_slots = 0; diff = 0\n",
    "        for c_str, grid in pred['output'].items():\n",
    "            ggrid = gold.get(c_str, None)\n",
    "            for d,row in enumerate(grid):\n",
    "                for s,cell in enumerate(row):\n",
    "                    total_slots += 1\n",
    "                    pred_pair = tuple(cell)\n",
    "                    if ggrid is None:\n",
    "                        # no gold -> skip\n",
    "                        continue\n",
    "                    gold_pair = tuple(ggrid[d][s])\n",
    "                    total += 1\n",
    "                    if pred_pair == gold_pair:\n",
    "                        correct += 1\n",
    "                    else:\n",
    "                        diff += 1\n",
    "        acc = correct / total if total>0 else float('nan')\n",
    "        accuracies.append(acc)\n",
    "        # simple hamming-like score = 1 - (diff / total_slots)\n",
    "        ham = 1.0 - (diff / total_slots) if total_slots>0 else float('nan')\n",
    "        hamming_scores.append(ham)\n",
    "    print('Per-instance accuracies (mean):', np.nanmean(accuracies), 'std:', np.nanstd(accuracies))\n",
    "    print('Hamming-like scores (mean):', np.nanmean(hamming_scores), 'std:', np.nanstd(hamming_scores))\n",
    "\n",
    "if instances and os.path.exists(PREDICT_DIR):\n",
    "    evaluate_predictions(instances, PREDICT_DIR)\n",
    "else:\n",
    "    print('No predictions to evaluate.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc02af7f",
   "metadata": {},
   "source": [
    "## 9) Next steps and improvements\n",
    "- **Masking & constraint-aware decoding**: restrict logits to feasible `(subject,teacher)` options per slot before selecting argmax. This prevents infeasible assignments.\n",
    "- **Post-processing repair**: run a greedy repair to enforce teacher exclusivity and subject-session counts.\n",
    "- **Use a proper GNN library** (PyTorch Geometric / DGL) to encode relations (class ↔ teacher ↔ subject) and message passing.\n",
    "- **Structured output models**: pointer networks, sequence models, or combinatorial decoders for better structured predictions.\n",
    "\n",
    "You're ready to run this notebook. Place your JSON instances in `../database` and execute the cells. The notebook will save predictions under `/mnt/data/predicted/`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
